{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600127d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… All libraries imported successfully!\n",
      "Dataset loaded: 235574 rows, 1 columns\n",
      "                       country;id;week.year;revenue;units\n",
      "KR;702234;03.2019;808                                08;1\n",
      "KR;702234;06.2019;1606                               80;2\n",
      "KR;3618438;08.2019;803                               40;1\n",
      "KR;3618438;09.2019;803                               40;1\n",
      "KR;3618438;09.2019;803                               40;1\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'week.year'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\khush\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3790\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3789\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3790\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3791\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:152\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:181\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'week.year'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 47\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28mprint\u001b[39m(df1\u001b[38;5;241m.\u001b[39mhead())\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# Convert week.year to proper date format\u001b[39;00m\n\u001b[1;32m---> 47\u001b[0m df1[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(\u001b[43mdf1\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweek.year\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mW.\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m# Rename columns\u001b[39;00m\n\u001b[0;32m     50\u001b[0m df2 \u001b[38;5;241m=\u001b[39m df1\u001b[38;5;241m.\u001b[39mcopy()\n",
      "File \u001b[1;32mc:\\Users\\khush\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\frame.py:3896\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3894\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   3895\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3896\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3897\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3898\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\Users\\khush\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3797\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3792\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3793\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3794\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3795\u001b[0m     ):\n\u001b[0;32m   3796\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3797\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3798\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3799\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3800\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3801\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3802\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'week.year'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import datetime as dt\n",
    "from datetime import timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import os # Import os module to create directories\n",
    "\n",
    "# Machine Learning Libraries\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    silhouette_score,\n",
    "    accuracy_score,\n",
    "    roc_auc_score,\n",
    "    roc_curve\n",
    ")\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"âœ… All libraries imported successfully!\")\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs('output', exist_ok=True)\n",
    "\n",
    "# ============================================================================\n",
    "# PART 2: LOAD AND PREPARE DATA (From Daniel's Original Code)\n",
    "# ============================================================================\n",
    "\n",
    "# Load the dataset\n",
    "# Option 1: Use UCI Online Retail dataset\n",
    "# df = pd.read_csv('data/online_retail.csv', encoding='unicode_escape')\n",
    "\n",
    "# Option 2: Use Daniel's sample data\n",
    "df1 = pd.read_csv('data/sales_asia.csv', sep=';')\n",
    "\n",
    "print(f\"Dataset loaded: {df1.shape[0]} rows, {df1.shape[1]} columns\")\n",
    "print(df1.head())\n",
    "\n",
    "# Convert week.year to proper date format\n",
    "# The week.year column is a float where the integer part is the week and the decimal part is the two-digit year (e.g., 3.20 means Week 3, Year 20)\n",
    "def parse_week_year_to_string(week_year_float):\n",
    "    week_int = int(week_year_float)\n",
    "    # Extract the fractional part and convert to a two-digit year\n",
    "    year_float_part = week_year_float - week_int\n",
    "    year_two_digits = int(round(year_float_part * 100)) # Round to handle floating point inaccuracies\n",
    "    # Construct the string as 'WEEK-YY-DAYOFWEEK' (using Monday as day 1)\n",
    "    return f\"{week_int}-{year_two_digits:02d}-1\"\n",
    "\n",
    "df1['date_string'] = df1['week.year'].apply(parse_week_year_to_string)\n",
    "df1['date'] = pd.to_datetime(df1['date_string'], format='%W-%y-%w')\n",
    "\n",
    "# Rename columns\n",
    "df2 = df1.copy()\n",
    "df2.rename(columns={'revenue': 'monetary'}, inplace=True)\n",
    "# Convert monetary to numeric, handling comma as decimal separator\n",
    "df2['monetary'] = df2['monetary'].str.replace(',', '.', regex=False).astype(float)\n",
    "df2 = df2[['country', 'id', 'monetary', 'units', 'date']]\n",
    "\n",
    "print(\"\\nâœ… Data preparation complete!\")\n",
    "print(df2.info())\n",
    "\n",
    "# ============================================================================\n",
    "# PART 3: RFM ANALYSIS (Original + Enhanced)\n",
    "# ============================================================================\n",
    "\n",
    "# Set analysis date (most recent date + 1 day)\n",
    "NOW = df2['date'].max() + timedelta(days=1)\n",
    "print(f\"\\nAnalysis Date: {NOW}\")\n",
    "\n",
    "# Calculate days since purchase\n",
    "df2['days_since_purchase'] = (NOW - df2['date']).dt.days\n",
    "\n",
    "# Filter last 365 days\n",
    "df3 = df2[df2['days_since_purchase'] <= 365].copy()\n",
    "\n",
    "# Create unique ID combining country and customer ID\n",
    "df3['id+'] = df3['country'].astype(str) + df3['id'].astype(str)\n",
    "\n",
    "print(f\"\\nFiltered to last 365 days: {df3.shape[0]} transactions\")\n",
    "\n",
    "# Aggregate RFM metrics\n",
    "rfm = df3.groupby(['id+', 'country', 'id']).agg(\n",
    "    recency=('days_since_purchase', 'min'),  # Recency\n",
    "    frequency=('id', 'count'),                 # Frequency (renamed from 'id' count)\n",
    "    monetary=('monetary', 'sum')              # Monetary\n",
    ").reset_index()\n",
    "\n",
    "# rfm.columns = ['id+', 'country', 'id', 'recency', 'frequency', 'monetary'] # This line is no longer needed with named aggregations\n",
    "\n",
    "print(f\"\\nâœ… RFM calculated for {len(rfm)} customers\")\n",
    "print(rfm.head())\n",
    "\n",
    "# ============================================================================\n",
    "# PART 4: RFM SCORING (Original)\n",
    "# ============================================================================\n",
    "\n",
    "# Calculate quintiles for R, F, M scores (1-5)\n",
    "rfm['r'] = pd.qcut(rfm['recency'], q=5, labels=[5, 4, 3, 2, 1])\n",
    "rfm['f'] = pd.qcut(rfm['frequency'].rank(method='first'), q=5, labels=[1, 2, 3, 4, 5])\n",
    "rfm['m'] = pd.qcut(rfm['monetary'], q=5, labels=[1, 2, 3, 4, 5])\n",
    "\n",
    "# Convert to integers\n",
    "rfm['r'] = rfm['r'].astype(int)\n",
    "rfm['f'] = rfm['f'].astype(int)\n",
    "rfm['m'] = rfm['m'].astype(int)\n",
    "\n",
    "# Create RFM score\n",
    "rfm['rfm_score'] = rfm['r'].astype(str) + rfm['f'].astype(str) + rfm['m'].astype(str)\n",
    "\n",
    "# Create FM score (simplified)\n",
    "rfm['fm'] = ((rfm['f'] + rfm['m']) / 2).apply(lambda x: math.trunc(x))\n",
    "\n",
    "print(\"\\nâœ… RFM Scores calculated!\")\n",
    "\n",
    "# ============================================================================\n",
    "# PART 5: SEGMENT MAPPING (Original)\n",
    "# ============================================================================\n",
    "\n",
    "# Create segment map\n",
    "segment_map = {\n",
    "    (5, 5): 'champions',\n",
    "    (5, 4): 'champions',\n",
    "    (4, 5): 'loyal customers',\n",
    "    (4, 4): 'loyal customers',\n",
    "    (5, 3): 'potential loyalists',\n",
    "    (4, 3): 'potential loyalists',\n",
    "    (5, 2): 'promising',\n",
    "    (4, 2): 'promising',\n",
    "    (3, 5): \"can't lose\",\n",
    "    (3, 4): \"can't lose\",\n",
    "    (5, 1): 'recent customers',\n",
    "    (4, 1): 'recent customers',\n",
    "    (3, 3): 'need attention',\n",
    "    (3, 2): 'about to sleep',\n",
    "    (2, 5): 'at risk',\n",
    "    (2, 4): 'at risk',\n",
    "    (2, 3): 'at risk',\n",
    "    (2, 2): 'hibernating',\n",
    "    (2, 1): 'hibernating',\n",
    "    (1, 5): 'lost',\n",
    "    (1, 4): 'lost',\n",
    "    (1, 3): 'lost',\n",
    "    (1, 2): 'lost',\n",
    "    (1, 1): 'lost'\n",
    "}\n",
    "\n",
    "rfm['segment'] = rfm[['r', 'fm']].apply(lambda x: segment_map.get((x['r'], x['fm']), 'other'), axis=1)\n",
    "\n",
    "print(\"\\nâœ… Customer segments assigned!\")\n",
    "print(rfm['segment'].value_counts())\n",
    "\n",
    "# ============================================================================\n",
    "# PART 6: ðŸ†• ENHANCEMENT 1 - K-MEANS CLUSTERING\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ðŸš€ ENHANCEMENT 1: K-MEANS CLUSTERING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Prepare data for clustering\n",
    "rfm_clustering = rfm[['recency', 'frequency', 'monetary']].copy()\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "rfm_scaled = scaler.fit_transform(rfm_clustering)\n",
    "\n",
    "# Elbow method to find optimal k\n",
    "inertias = []\n",
    "silhouette_scores = []\n",
    "K_range = range(2, 11)\n",
    "\n",
    "for k in K_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    kmeans.fit(rfm_scaled)\n",
    "    inertias.append(kmeans.inertia_)\n",
    "    silhouette_scores.append(silhouette_score(rfm_scaled, kmeans.labels_))\n",
    "\n",
    "# Plot elbow curve\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].plot(K_range, inertias, 'bo-', linewidth=2, markersize=8)\n",
    "axes[0].set_xlabel('Number of Clusters (k)', fontsize=12)\n",
    "axes[0].set_ylabel('Inertia', fontsize=12)\n",
    "axes[0].set_title('Elbow Method For Optimal k', fontsize=14, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(K_range, silhouette_scores, 'ro-', linewidth=2, markersize=8)\n",
    "axes[1].set_xlabel('Number of Clusters (k)', fontsize=12)\n",
    "axes[1].set_ylabel('Silhouette Score', fontsize=12)\n",
    "axes[1].set_title('Silhouette Analysis', fontsize=14, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('output/kmeans_optimization.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Apply K-Means with optimal k (typically 5)\n",
    "optimal_k = 5\n",
    "kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
    "rfm['ml_cluster'] = kmeans.fit_predict(rfm_scaled)\n",
    "\n",
    "# Analyze clusters\n",
    "print(f\"\\nâœ… K-Means clustering complete with k={optimal_k}\")\n",
    "print(f\"Silhouette Score: {silhouette_score(rfm_scaled, rfm['ml_cluster']):.3f}\\n\")\n",
    "\n",
    "cluster_analysis = rfm.groupby('ml_cluster').agg({\n",
    "    'recency': ['mean', 'min', 'max'],\n",
    "    'frequency': ['mean', 'min', 'max'],\n",
    "    'monetary': ['mean', 'sum', 'count']\n",
    "}).round(2)\n",
    "\n",
    "print(\"Cluster Analysis:\")\n",
    "print(cluster_analysis)\n",
    "\n",
    "# Visualize clusters\n",
    "fig = plt.figure(figsize=(15, 5))\n",
    "\n",
    "# 2D scatter: Recency vs Frequency\n",
    "ax1 = fig.add_subplot(131)\n",
    "scatter1 = ax1.scatter(rfm['recency'], rfm['frequency'],\n",
    "                       c=rfm['ml_cluster'], cmap='viridis',\n",
    "                       s=50, alpha=0.6, edgecolors='black', linewidth=0.5)\n",
    "ax1.set_xlabel('Recency (days)', fontsize=11)\n",
    "ax1.set_ylabel('Frequency (orders)', fontsize=11)\n",
    "ax1.set_title('Clusters: Recency vs Frequency', fontsize=12, fontweight='bold')\n",
    "plt.colorbar(scatter1, ax=ax1, label='Cluster')\n",
    "\n",
    "# 2D scatter: Frequency vs Monetary\n",
    "ax2 = fig.add_subplot(132)\n",
    "scatter2 = ax2.scatter(rfm['frequency'], rfm['monetary'],\n",
    "                       c=rfm['ml_cluster'], cmap='viridis',\n",
    "                       s=50, alpha=0.6, edgecolors='black', linewidth=0.5)\n",
    "ax2.set_xlabel('Frequency (orders)', fontsize=11)\n",
    "ax2.set_ylabel('Monetary (value)', fontsize=11)\n",
    "ax2.set_title('Clusters: Frequency vs Monetary', fontsize=12, fontweight='bold')\n",
    "plt.colorbar(scatter2, ax=ax2, label='Cluster')\n",
    "\n",
    "# 3D scatter\n",
    "ax3 = fig.add_subplot(133, projection='3d')\n",
    "scatter3 = ax3.scatter(rfm['recency'], rfm['frequency'], rfm['monetary'],\n",
    "                       c=rfm['ml_cluster'], cmap='viridis',\n",
    "                       s=50, alpha=0.6, edgecolors='black', linewidth=0.5)\n",
    "ax3.set_xlabel('Recency', fontsize=10)\n",
    "ax3.set_ylabel('Frequency', fontsize=10)\n",
    "ax3.set_zlabel('Monetary', fontsize=10)\n",
    "ax3.set_title('3D Cluster Visualization', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('output/cluster_visualization.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# ============================================================================\n",
    "# PART 7: ðŸ†• ENHANCEMENT 2 - CHURN PREDICTION WITH RANDOM FOREST\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ðŸš€ ENHANCEMENT 2: CHURN PREDICTION MODEL\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Define churn (customers who haven't purchased in 180+ days)\n",
    "churn_threshold = 180\n",
    "rfm['is_churned'] = (rfm['recency'] > churn_threshold).astype(int)\n",
    "\n",
    "print(f\"\\nChurn Definition: Recency > {churn_threshold} days\")\n",
    "print(f\"Churned Customers: {rfm['is_churned'].sum()} ({rfm['is_churned'].mean()*100:.1f}%)\")\n",
    "print(f\"Active Customers: {(1-rfm['is_churned']).sum()} ({(1-rfm['is_churned']).mean()*100:.1f}%)\")\n",
    "\n",
    "# Prepare features\n",
    "feature_cols = ['recency', 'frequency', 'monetary', 'r', 'f', 'm', 'fm']\n",
    "X = rfm[feature_cols]\n",
    "y = rfm['is_churned']\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining set: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "\n",
    "# Train Random Forest Classifier\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    random_state=42,\n",
    "    class_weight='balanced',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = rf_model.predict(X_test)\n",
    "y_pred_proba = rf_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluate model\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"MODEL PERFORMANCE\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "print(f\"\\nAccuracy: {accuracy_score(y_test, y_pred):.3f}\")\n",
    "print(f\"ROC-AUC Score: {roc_auc_score(y_test, y_pred_proba):.3f}\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['Active', 'Churned']))\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "\n",
    "# Visualize confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=['Active', 'Churned'],\n",
    "            yticklabels=['Active', 'Churned'],\n",
    "            cbar_kws={'label': 'Count'})\n",
    "plt.xlabel('Predicted', fontsize=12)\n",
    "plt.ylabel('Actual', fontsize=12)\n",
    "plt.title('Confusion Matrix - Churn Prediction', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('output/confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Feature importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'importance': rf_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nFeature Importance:\")\n",
    "print(feature_importance)\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=feature_importance, x='importance', y='feature', palette='viridis')\n",
    "plt.xlabel('Importance Score', fontsize=12)\n",
    "plt.ylabel('Feature', fontsize=12)\n",
    "plt.title('Feature Importance for Churn Prediction', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('output/feature_importance.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Add churn probability to all customers\n",
    "rfm['churn_probability'] = rf_model.predict_proba(X)[:, 1]\n",
    "\n",
    "# Create churn risk categories\n",
    "rfm['churn_risk'] = pd.cut(\n",
    "    rfm['churn_probability'],\n",
    "    bins=[0, 0.3, 0.7, 1.0],\n",
    "    labels=['Low Risk', 'Medium Risk', 'High Risk']\n",
    ")\n",
    "\n",
    "print(\"\\nâœ… Churn prediction complete!\")\n",
    "print(\"\\nChurn Risk Distribution:\")\n",
    "print(rfm['churn_risk'].value_counts())\n",
    "\n",
    "# ROC Curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, linewidth=2, label=f'ROC Curve (AUC = {roc_auc_score(y_test, y_pred_proba):.3f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random Classifier')\n",
    "plt.xlabel('False Positive Rate', fontsize=12)\n",
    "plt.ylabel('True Positive Rate', fontsize=12)\n",
    "plt.title('ROC Curve - Churn Prediction Model', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('output/roc_curve.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# ============================================================================\n",
    "# PART 8: ðŸ†• ENHANCEMENT 3 - ADVANCED VISUALIZATIONS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ðŸš€ ENHANCEMENT 3: ADVANCED VISUALIZATIONS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Churn probability by segment\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "ax1 = plt.subplot(121)\n",
    "segment_churn = rfm.groupby('segment')['churn_probability'].mean().sort_values(ascending=False)\n",
    "segment_churn.plot(kind='barh', color='coral', edgecolor='black', ax=ax1)\n",
    "ax1.set_xlabel('Average Churn Probability', fontsize=11)\n",
    "ax1.set_ylabel('Customer Segment', fontsize=11)\n",
    "ax1.set_title('Churn Risk by Segment', fontsize=13, fontweight='bold')\n",
    "ax1.grid(axis='x', alpha=0.3)\n",
    "\n",
    "ax2 = plt.subplot(122)\n",
    "sns.boxplot(data=rfm, y='segment', x='churn_probability', palette='Set2', ax=ax2)\n",
    "ax2.set_xlabel('Churn Probability', fontsize=11)\n",
    "ax2.set_ylabel('Customer Segment', fontsize=11)\n",
    "ax2.set_title('Churn Probability Distribution by Segment', fontsize=13, fontweight='bold')\n",
    "ax2.grid(axis='x', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('output/churn_by_segment.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Revenue analysis\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "ax1 = plt.subplot(121)\n",
    "segment_revenue = rfm.groupby('segment')['monetary'].sum().sort_values(ascending=False)\n",
    "segment_revenue.plot(kind='bar', color='steelblue', edgecolor='black', ax=ax1)\n",
    "ax1.set_xlabel('Customer Segment', fontsize=11)\n",
    "ax1.set_ylabel('Total Revenue', fontsize=11)\n",
    "ax1.set_title('Revenue Contribution by Segment', fontsize=13, fontweight='bold')\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "ax2 = plt.subplot(122)\n",
    "at_risk_revenue = rfm.groupby('churn_risk')['monetary'].sum()\n",
    "colors = ['green', 'orange', 'red']\n",
    "at_risk_revenue.plot(kind='pie', autopct='%1.1f%%', colors=colors,\n",
    "                     startangle=90, ax=ax2,\n",
    "                     labels=['Low Risk', 'Medium Risk', 'High Risk'])\n",
    "ax2.set_ylabel('')\n",
    "ax2.set_title('Revenue Distribution by Churn Risk', fontsize=13, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('output/revenue_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Customer distribution\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Segment distribution\n",
    "segment_counts = rfm['segment'].value_counts()\n",
    "axes[0,0].pie(segment_counts, labels=segment_counts.index, autopct='%1.1f%%', startangle=90)\n",
    "axes[0,0].set_title('Customer Distribution by Segment', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Cluster distribution\n",
    "cluster_counts = rfm['ml_cluster'].value_counts().sort_index()\n",
    "axes[0,1].bar(cluster_counts.index, cluster_counts.values, color='skyblue', edgecolor='black')\n",
    "axes[0,1].set_xlabel('ML Cluster', fontsize=11)\n",
    "axes[0,1].set_ylabel('Number of Customers', fontsize=11)\n",
    "axes[0,1].set_title('Customer Distribution by ML Cluster', fontsize=12, fontweight='bold')\n",
    "axes[0,1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# RFM heatmap\n",
    "rfm_pivot = rfm.groupby(['r', 'fm']).size().unstack(fill_value=0)\n",
    "sns.heatmap(rfm_pivot, annot=True, fmt='d', cmap='YlOrRd', ax=axes[1,0], cbar_kws={'label': 'Count'})\n",
    "axes[1,0].set_xlabel('FM Score', fontsize=11)\n",
    "axes[1,0].set_ylabel('R Score', fontsize=11)\n",
    "axes[1,0].set_title('Customer Heatmap: R vs FM Scores', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Churn risk distribution\n",
    "churn_risk_counts = rfm['churn_risk'].value_counts()\n",
    "axes[1,1].barh(churn_risk_counts.index, churn_risk_counts.values,\n",
    "               color=['green', 'orange', 'red'], edgecolor='black')\n",
    "axes[1,1].set_xlabel('Number of Customers', fontsize=11)\n",
    "axes[1,1].set_ylabel('Churn Risk Category', fontsize=11)\n",
    "axes[1,1].set_title('Customers by Churn Risk', fontsize=12, fontweight='bold')\n",
    "axes[1,1].grid(axis='x', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('output/customer_distribution.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ… Advanced visualizations created and saved!\")\n",
    "\n",
    "# ============================================================================\n",
    "# PART 9: ðŸ†• ENHANCEMENT 4 - BUSINESS RECOMMENDATIONS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ðŸš€ ENHANCEMENT 4: STRATEGIC BUSINESS RECOMMENDATIONS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Calculate business metrics\n",
    "total_customers = len(rfm)\n",
    "total_revenue = rfm['monetary'].sum()\n",
    "avg_order_value = rfm['monetary'].mean()\n",
    "high_risk_customers = len(rfm[rfm['churn_risk'] == 'High Risk'])\n",
    "high_risk_revenue = rfm[rfm['churn_risk'] == 'High Risk']['monetary'].sum()\n",
    "\n",
    "print(f\"\\nðŸ“Š KEY BUSINESS METRICS\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Total Customers: {total_customers:,}\")\n",
    "print(f\"Total Revenue: ${total_revenue:,.2f}\")\n",
    "print(f\"Average Customer Value: ${avg_order_value:,.2f}\")\n",
    "print(f\"High-Risk Customers: {high_risk_customers:,} ({high_risk_customers/total_customers*100:.1f}%)\")\n",
    "print(f\"At-Risk Revenue: ${high_risk_revenue:,.2f} ({high_risk_revenue/total_revenue*100:.1f}%)\")\n",
    "\n",
    "# Segment-specific recommendations\n",
    "recommendations = {\n",
    "    'champions': {\n",
    "        'action': 'VIP Treatment',\n",
    "        'strategy': 'Early access to new products, exclusive rewards, personal account manager',\n",
    "        'priority': 'HIGH',\n",
    "        'expected_impact': 'Increase LTV by 25%'\n",
    "    },\n",
    "    'loyal customers': {\n",
    "        'action': 'Loyalty Program',\n",
    "        'strategy': 'Referral bonuses, birthday discounts, tier-based rewards',\n",
    "        'priority': 'HIGH',\n",
    "        'expected_impact': 'Boost repeat purchases by 20%'\n",
    "    },\n",
    "    'potential loyalists': {\n",
    "        'action': 'Nurture Campaign',\n",
    "        'strategy': 'Product recommendations, email engagement, limited-time offers',\n",
    "        'priority': 'MEDIUM',\n",
    "        'expected_impact': 'Convert 30% to loyal customers'\n",
    "    },\n",
    "    'at risk': {\n",
    "        'action': 'Win-Back Campaign',\n",
    "        'strategy': 'Personalized emails, exclusive discount (15-20%), survey feedback',\n",
    "        'priority': 'HIGH',\n",
    "        'expected_impact': 'Recover 25% of at-risk customers'\n",
    "    },\n",
    "    \"can't lose\": {\n",
    "        'action': 'Urgent Reactivation',\n",
    "        'strategy': 'Phone outreach, significant discount, account review meeting',\n",
    "        'priority': 'URGENT',\n",
    "        'expected_impact': 'Prevent $500K+ revenue loss'\n",
    "    },\n",
    "    'hibernating': {\n",
    "        'action': 'Reengagement',\n",
    "        'strategy': 'Multi-channel campaign, new product showcase, limited-time offers',\n",
    "        'priority': 'MEDIUM',\n",
    "        'expected_impact': 'Reactivate 15% of hibernating customers'\n",
    "    },\n",
    "    'lost': {\n",
    "        'action': 'Last Attempt',\n",
    "        'strategy': 'Survey for feedback, major discount code, re-onboarding offer',\n",
    "        'priority': 'LOW',\n",
    "        'expected_impact': 'Recover 5-10% of lost customers'\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"\\nðŸŽ¯ SEGMENT-SPECIFIC RECOMMENDATIONS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for segment, details in recommendations.items():\n",
    "    segment_data = rfm[rfm['segment'] == segment]\n",
    "    if len(segment_data) > 0:\n",
    "        print(f\"\\n{segment.upper()}\")\n",
    "        print(f\"  Customers: {len(segment_data):,}\")\n",
    "        print(f\"  Revenue: ${segment_data['monetary'].sum():,.2f}\")\n",
    "        print(f\"  Priority: {details['priority']}\")\n",
    "        print(f\"  Action: {details['action']}\")\n",
    "        print(f\"  Strategy: {details['strategy']}\")\n",
    "        print(f\"  Expected Impact: {details['expected_impact']}\")\n",
    "\n",
    "# Top priority customers\n",
    "print(f\"\\nðŸ”¥ TOP PRIORITY ACTIONS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# High-value at-risk customers\n",
    "high_value_at_risk = rfm[\n",
    "    (rfm['churn_risk'] == 'High Risk') &\n",
    "    (rfm['monetary'] > rfm['monetary'].quantile(0.75))\n",
    "].sort_values('monetary', ascending=False).head(10)\n",
    "\n",
    "print(f\"\\n1. HIGH-VALUE AT-RISK CUSTOMERS (Top 10)\")\n",
    "print(f\"   Total at Risk: ${high_value_at_risk['monetary'].sum():,.2f}\")\n",
    "print(\"   Immediate Action Required!\")\n",
    "print(high_value_at_risk[['id', 'segment', 'recency', 'frequency', 'monetary', 'churn_probability']])\n",
    "\n",
    "# Champions to retain\n",
    "champions = rfm[rfm['segment'] == 'champions'].sort_values('monetary', ascending=False).head(10)\n",
    "if len(champions) > 0:\n",
    "    print(f\"\\n2. TOP CHAMPIONS (Protect at All Costs)\")\n",
    "    print(f\"   Total Value: ${champions['monetary'].sum():,.2f}\")\n",
    "    print(champions[['id', 'recency', 'frequency', 'monetary', 'churn_probability']])\n",
    "\n",
    "# Potential to loyal conversion\n",
    "potential_loyalists = rfm[\n",
    "    (rfm['segment'] == 'potential loyalists') &\n",
    "    (rfm['churn_probability'] < 0.3)\n",
    "].sort_values('monetary', ascending=False).head(10)\n",
    "\n",
    "if len(potential_loyalists) > 0:\n",
    "    print(f\"\\n3. POTENTIAL LOYALISTS TO CONVERT\")\n",
    "    print(f\"   Conversion Opportunity: ${potential_loyalists['monetary'].sum():,.2f}\")\n",
    "    print(potential_loyalists[['id', 'recency', 'frequency', 'monetary', 'churn_probability']])\n",
    "\n",
    "# ============================================================================\n",
    "# PART 10: EXPORT ENHANCED DATASET\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ðŸ’¾ EXPORTING ENHANCED DATASET\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Add calculated fields for Power BI\n",
    "rfm['customer_lifetime_value'] = rfm['monetary'] * rfm['frequency']\n",
    "rfm['average_order_value'] = rfm['monetary'] / rfm['frequency']\n",
    "rfm['days_since_first_purchase'] = 365 - rfm['recency']\n",
    "\n",
    "# Create final output\n",
    "output_columns = [\n",
    "    'id', 'id+', 'country',\n",
    "    'recency', 'frequency', 'monetary',\n",
    "    'r', 'f', 'm', 'fm', 'rfm_score',\n",
    "    'segment', 'ml_cluster',\n",
    "    'is_churned', 'churn_probability', 'churn_risk',\n",
    "    'customer_lifetime_value', 'average_order_value', 'days_since_first_purchase'\n",
    "]\n",
    "\n",
    "rfm_output = rfm[output_columns].copy()\n",
    "\n",
    "# Export to CSV\n",
    "rfm_output.to_csv('output/rfm_enhanced.csv', index=False, float_format='%.2f')\n",
    "\n",
    "print(f\"\\nâœ… Enhanced dataset exported: output/rfm_enhanced.csv\")\n",
    "print(f\"   Total customers: {len(rfm_output):,}\")\n",
    "print(f\"   Total features: {len(output_columns)}\")\n",
    "\n",
    "# Summary statistics\n",
    "print(f\"\\nðŸ“ˆ FINAL SUMMARY STATISTICS\")\n",
    "print(\"=\"*70)\n",
    "print(rfm_output.describe())\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"âœ… PROJECT COMPLETE!\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nNext Steps:\")\n",
    "print(\"1. Open Power BI Desktop\")\n",
    "print(\"2. Import: output/rfm_enhanced.csv\")\n",
    "print(\"3. Create dashboards using the guide in PROJECT_GUIDE.md\")\n",
    "print(\"4. Update your GitHub repository\")\n",
    "print(\"5. Add to resume with metrics!\")\n",
    "\n",
    "# Save summary report\n",
    "summary = f\"\"\"\n",
    "CUSTOMER SEGMENTATION ENHANCED - PROJECT SUMMARY\n",
    "Generated: {dt.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\n",
    "DATASET OVERVIEW\n",
    "- Total Customers: {total_customers:,}\n",
    "- Total Revenue: ${total_revenue:,.2f}\n",
    "- Analysis Period: Last 365 days\n",
    "- Countries: {rfm['country'].nunique()}\n",
    "\n",
    "MACHINE LEARNING RESULTS\n",
    "- K-Means Clusters: {optimal_k}\n",
    "- Silhouette Score: {silhouette_score(rfm_scaled, rfm['ml_cluster']):.3f}\n",
    "- Churn Model Accuracy: {accuracy_score(y_test, y_pred):.3f}\n",
    "- ROC-AUC Score: {roc_auc_score(y_test, y_pred_proba):.3f}\n",
    "\n",
    "BUSINESS METRICS\n",
    "- High-Risk Customers: {high_risk_customers:,} ({high_risk_customers/total_customers*100:.1f}%)\n",
    "- At-Risk Revenue: ${high_risk_revenue:,.2f}\n",
    "- Average Customer Value: ${avg_order_value:,.2f}\n",
    "\n",
    "TOP SEGMENTS\n",
    "{rfm['segment'].value_counts().head()}\n",
    "\n",
    "FILES GENERATED\n",
    "- rfm_enhanced.csv (main dataset)\n",
    "- kmeans_optimization.png\n",
    "- cluster_visualization.png\n",
    "- confusion_matrix.png\n",
    "- feature_importance.png\n",
    "- roc_curve.png\n",
    "- churn_by_segment.png\n",
    "- revenue_analysis.png\n",
    "- customer_distribution.png\n",
    "\"\"\"\n",
    "\n",
    "with open('output/project_summary.txt', 'w') as f:\n",
    "    f.write(summary)\n",
    "\n",
    "print(\"\\nðŸ“„ Project summary saved: output/project_summary.txt\")\n",
    "print(\"\\nðŸŽ‰ All Done! Happy Analyzing!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
